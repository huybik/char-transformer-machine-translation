{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char level transformer for Machine translation\n",
    "To do translation, we'll use sequence-to-sequence model: an encoder encode the input and decoder using both the encode and raw output to predict. \n",
    "\n",
    "This is similar to char-transformer for language modeling, but we will add an encoder, and in place of top-k output when we generate text we'll use top-1 beam search output.\n",
    "\n",
    "<img src='images/transformer.png' width=400>\n",
    "\n",
    "<a href=\"https://www.kaggle.com/hungnm/englishvietnamese-translation\">English-Vietnamese HungMN Kaggle</a>\n",
    "\n",
    "<a href=\"https://github.com/vietai/SAT\">English-Vietnamese dataset VietAI SAT</a>\n",
    "\n",
    "<a href=\"https://nlp.stanford.edu/projects/nmt/\">English-Vietnamese dataset IWSLT 15</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    '''\n",
    "    Dataset is a iterable that returns input and target sentence. It adds <sos> at the begining, and <eos> at the end, \n",
    "    and filling in <pad> if sentence length is less than pre-defined value.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, x, y, sequence_len, encoder=None):\n",
    "        # data in the type of pairs of sentence\n",
    "        data = ''.join(x+y)\n",
    "        # from collections import Counter\n",
    "        # vocab_size = 250\n",
    "\n",
    "        # ct = Counter(data)\n",
    "        # include = sorted(ct, key=ct.get, reverse=True)\n",
    "        # if len(include)>vocab_size: include = include[:vocab_size]\n",
    "        # rule = ''.join(include)\n",
    "        chars = ['<pad>'] +['<sos>'] + ['<eos>'] + sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "        print('data has %d characters, %d unique chars, %d sentences.' % (data_size, len(chars), len(x)))\n",
    "        print('sentence length nine_nine_percentile: %d' % (sequence_len))\n",
    "        \n",
    "        self.x, self.y = x, y\n",
    "        self.ch2i = {ch:i for i,ch in enumerate(chars)}\n",
    "        self.i2ch = {i:ch for i,ch in enumerate(chars)}\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sequence_len = sequence_len\n",
    "        self.encoder=encoder\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x) # len x = y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        indx = self.padding([self.ch2i[ch] for ch in self.x[idx]] + [self.ch2i['<eos>']])\n",
    "        indy = [self.ch2i['<sos>']] + self.padding([self.ch2i[ch] for ch in self.y[idx]] + [self.ch2i['<eos>']])\n",
    "\n",
    "        x = torch.tensor(indx, dtype=torch.long)\n",
    "        y = torch.tensor(indy, dtype=torch.long)\n",
    "\n",
    "        return x,y\n",
    "                                                                                                                               \n",
    "    def padding(self, string):\n",
    "        if len(string)<self.sequence_len:\n",
    "            string =  string + [0]*(self.sequence_len - len(string))\n",
    "        else:\n",
    "            string = string[:self.sequence_len -1] + [self.ch2i['<eos>']]\n",
    "                   \n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_len = 128\n",
    "min_len = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # process and save data\n",
    "# from utils.pre_processing import *\n",
    "# from utils.utils import *\n",
    "# en, vi = list(), list()\n",
    "# paths = [\"data/vietaisat/\",\"data/hungnm/\",\"data/iwslt15/\"]\n",
    "# for path in paths:\n",
    "#     x = open(path+\"en.txt\", encoding='utf-8').read().split(\"\\n\")\n",
    "#     y = open(path+\"vi.txt\", encoding='utf-8').read().split(\"\\n\")\n",
    "#     x,y = pre_processing(x, y, min_length=min_len, max_length=sequence_len) # remove sentence less than 4 characters\n",
    "#     en += x\n",
    "#     vi += y\n",
    "    \n",
    "# path = \"data/cleaned/\"\n",
    "# pickle(path+\"en\", en)\n",
    "# pickle(path+\"vi\", vi)\n",
    "# # nine_nine_percentile = int(np.percentile([len(sen) for sen in vi],99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some last sentences\n",
      " | \n",
      "Didier Sornette How we can predict the next financial crisis | Paul Pholeros Làm sao để bớt nghèo khổ ? Hãy sửa nhà\n",
      "Thank you very much for your time . | Rất cảm ơn đã lắng nghe .\n",
      "It s manmade and can be overcome and eradicated by the actions of human beings .  | Nó là do con người và có thể ngăn chặn và diệt trừ bởi hành động của con người . \n"
     ]
    }
   ],
   "source": [
    "# Load saved data\n",
    "from utils.utils import *\n",
    "from utils.pre_processing import *\n",
    "\n",
    "path = \"data/cleaned/\"\n",
    "en = pickle(path+\"en\")\n",
    "vi = pickle(path+\"vi\")\n",
    "en,vi = pre_processing(en, vi, min_length=min_len, max_length=sequence_len) # clip sentences\n",
    "for i in range(-1,-5,-1):\n",
    "    print(en[i],'|',vi[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 292892120 characters, 206 unique chars, 2448155 sentences.\n",
      "sentence length nine_nine_percentile: 128\n",
      "sample tensors  (tensor([ 69,  54,  59,   3,  67,  66,  54,   3,  57,  97,  59,  52,   3, 107,\n",
      "        139,  65,   3,  59,  52, 115, 177,  54,   3,  62,  66,  93,  65,   3,\n",
      "         63,  89,  48,   3,  65,  63,  60,  59,  52,   3,  65, 187,   3,  48,\n",
      "         53, 169,  54,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0]), tensor([ 1, 34, 57, 50, 46, 64, 50,  3, 61, 66, 65,  3, 65, 53, 50,  3, 49, 66,\n",
      "        64, 65, 61, 46, 59,  3, 54, 59,  3, 65, 53, 50,  3, 47, 63, 60, 60, 58,\n",
      "         3, 48, 57, 60, 64, 50, 65,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0]))\n",
      "vocab:  {'<pad>': 0, '<sos>': 1, '<eos>': 2, ' ': 3, '!': 4, \"'\": 5, ',': 6, '.': 7, '0': 8, '1': 9, '2': 10, '3': 11, '4': 12, '5': 13, '6': 14, '7': 15, '8': 16, '9': 17, '?': 18, 'A': 19, 'B': 20, 'C': 21, 'D': 22, 'E': 23, 'F': 24, 'G': 25, 'H': 26, 'I': 27, 'J': 28, 'K': 29, 'L': 30, 'M': 31, 'N': 32, 'O': 33, 'P': 34, 'Q': 35, 'R': 36, 'S': 37, 'T': 38, 'U': 39, 'V': 40, 'W': 41, 'X': 42, 'Y': 43, 'Z': 44, '_': 45, 'a': 46, 'b': 47, 'c': 48, 'd': 49, 'e': 50, 'f': 51, 'g': 52, 'h': 53, 'i': 54, 'j': 55, 'k': 56, 'l': 57, 'm': 58, 'n': 59, 'o': 60, 'p': 61, 'q': 62, 'r': 63, 's': 64, 't': 65, 'u': 66, 'v': 67, 'w': 68, 'x': 69, 'y': 70, 'z': 71, 'À': 72, 'Á': 73, 'Â': 74, 'Ã': 75, 'È': 76, 'É': 77, 'Ê': 78, 'Ì': 79, 'Í': 80, 'Ò': 81, 'Ó': 82, 'Ô': 83, 'Õ': 84, 'Ù': 85, 'Ú': 86, 'Ý': 87, 'à': 88, 'á': 89, 'â': 90, 'ã': 91, 'è': 92, 'é': 93, 'ê': 94, 'ì': 95, 'í': 96, 'ò': 97, 'ó': 98, 'ô': 99, 'õ': 100, 'ù': 101, 'ú': 102, 'ý': 103, 'Ă': 104, 'ă': 105, 'Đ': 106, 'đ': 107, 'Ĩ': 108, 'ĩ': 109, 'Ũ': 110, 'ũ': 111, 'Ơ': 112, 'ơ': 113, 'Ư': 114, 'ư': 115, 'Ạ': 116, 'ạ': 117, 'Ả': 118, 'ả': 119, 'Ấ': 120, 'ấ': 121, 'Ầ': 122, 'ầ': 123, 'Ẩ': 124, 'ẩ': 125, 'Ẫ': 126, 'ẫ': 127, 'Ậ': 128, 'ậ': 129, 'Ắ': 130, 'ắ': 131, 'Ằ': 132, 'ằ': 133, 'Ẳ': 134, 'ẳ': 135, 'Ẵ': 136, 'ẵ': 137, 'Ặ': 138, 'ặ': 139, 'Ẹ': 140, 'ẹ': 141, 'Ẻ': 142, 'ẻ': 143, 'Ẽ': 144, 'ẽ': 145, 'Ế': 146, 'ế': 147, 'Ề': 148, 'ề': 149, 'Ể': 150, 'ể': 151, 'Ễ': 152, 'ễ': 153, 'Ệ': 154, 'ệ': 155, 'Ỉ': 156, 'ỉ': 157, 'Ị': 158, 'ị': 159, 'Ọ': 160, 'ọ': 161, 'Ỏ': 162, 'ỏ': 163, 'Ố': 164, 'ố': 165, 'Ồ': 166, 'ồ': 167, 'Ổ': 168, 'ổ': 169, 'Ỗ': 170, 'ỗ': 171, 'Ộ': 172, 'ộ': 173, 'Ớ': 174, 'ớ': 175, 'Ờ': 176, 'ờ': 177, 'Ở': 178, 'ở': 179, 'Ỡ': 180, 'ỡ': 181, 'Ợ': 182, 'ợ': 183, 'Ụ': 184, 'ụ': 185, 'Ủ': 186, 'ủ': 187, 'Ứ': 188, 'ứ': 189, 'Ừ': 190, 'ừ': 191, 'Ử': 192, 'ử': 193, 'Ữ': 194, 'ữ': 195, 'Ự': 196, 'ự': 197, 'Ỳ': 198, 'ỳ': 199, 'Ỵ': 200, 'ỵ': 201, 'Ỷ': 202, 'ỷ': 203, 'Ỹ': 204, 'ỹ': 205}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = CharDataset(vi, en, sequence_len=sequence_len)\n",
    "\n",
    "print('sample tensors ', next(iter(dataset)))\n",
    "print(\"vocab: \", dataset.ch2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.encode_decode_transformer import Transformer, TransformerConfig\n",
    "from utils.trainer import Trainer, TrainerConfig\n",
    "tconfig = TrainerConfig(max_epochs=1, batch_size=16, learning_rate=6e-4, grad_norm_clip=1.0, device='cuda',\n",
    "                       lr_decay=True, warmup_tokens=5000, ckpt_n_print_iter=4000, ckpt_path='checkpoint/transformer_vn_en_char')\n",
    "\n",
    "mconfig = TransformerConfig(vocab_size=dataset.vocab_size, sequence_len=dataset.sequence_len, embed_dim=256,\n",
    "                           n_block=8, n_head=8, device=tconfig.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/04/2021 11:58:26 - INFO - model.encode_decode_transformer -   number of parameters: 1.487002e+07\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(mconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"hôm nay trông bạn thật đẹp!\",\n",
    "           \"đưa tôi một chai nước, tôi khát khô cổ rồi.\",\n",
    "           \"thời tiết hôm nay thật đẹp!\",\n",
    "           \"bạn đã ăn sáng chưa?\",\n",
    "           \"chúng ta sẽ khởi hành vào rạng sáng mai.\"\n",
    "            ]\n",
    "trainer = Trainer(model, dataset, tconfig, test_dataset=sentences, collate=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load pre-trained weights\n",
    "# from utils.utils import pickle\n",
    "# model.load_state_dict(pickle(tconfig.ckpt_path)) # load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\"Hôm nay cái áo bạn mặc trông thật đẹp, nó bao tiền vậy?\",\n",
    "           \"Nhà tôi có 1 con zombie trà sữa\",\n",
    "           \"nó đáng yêu nhưng rất sợ sấm và bóng tối\",\n",
    "           \"Hôm nay nhìn Amy không khác gì tranh vẽ\",\n",
    "           \"dù Amy chưa ăn sáng\",\n",
    "           \"Chúng ta sẽ khởi hành vào rạng sáng mai, hãy chuẩn bị kỹ.\"\n",
    "          ]\n",
    "result = model.generate_output(samples, dataset, top_k=5, print_process=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmarking using bleu score\n",
    "path = \"data/iwslt15/\"\n",
    "en = open(path+\"tst2013.en.txt\", encoding='utf-8').read().split(\"\\n\")\n",
    "vi = open(path+\"tst2013.vi.txt\", encoding='utf-8').read().split(\"\\n\")\n",
    "en,vi = pre_processing(en, vi, min_length=min_len, max_length=sequence_len) # remove sentence less than 4 characters\n",
    "\n",
    "result = model.generate_output(vi, dataset, top_k=5, print_process=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import *\n",
    "score, references, candidates = bleu_score(en, result)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(vi)):\n",
    "    print(vi[i], \" | \", en[i], \" | \", result[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
